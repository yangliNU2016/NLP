import nltk
import re
import numpy as np

# Map the word types to their indices generated by the order of occurance in file
file1 = nltk.data.path[0] + '/corpora/brown/brown_vocab_100.txt'
words = dict()
with open(file1) as f:
	for index, line in enumerate(f):
		words[line.rstrip()] = index
		
# Initialize a counter that counts how many each of the words 	
counts = np.zeros((len(words)))

# Collect the words into an array 
file2 = nltk.data.path[0] + '/corpora/brown/brown_100.txt'
wds = []
with open(file2) as g:
	for line in g:
		wdsEachLine = line[:-1].lower().split()
		wdsEachLine.append('</s>')
		wds.extend(wdsEachLine)

# Update the counter for each of the words and store it in the counter initialized above  
for wd in wds:
	counts[words[wd]] += 1

# Calculate the probability for each of the words occurred in the text
probs = counts / np.sum(counts)

# Write them into a file
target = open('unigram_probs.txt', 'w')
target.write(str(probs))
target.close()

file3 = nltk.data.path[0] + '/corpora/brown/toy_corpus.txt'
target1 = open('unigram_eval.txt', 'w')
with open(file3) as h:
	for line in h:
		wdsInSentence = line[:-1].lower().split()
		wdsInSentence.append('</s>')
		sentProb = 1
		for wd in wdsInSentence:
			sentProb *= probs[words[wd]]
		perplexity = 1 / (pow(sentProb, 1.0 / len(wdsInSentence)))
		target1.write(str(perplexity) + '\n')
	target1.close()
